{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 13:06:54 WARN Utils: Your hostname, ubuntu-hadoop resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/02/25 13:06:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 13:06:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/25 13:06:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local[*]\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "import pandas as pd  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.getOrCreate()\n",
    "from  pyspark.sql.functions import split\n",
    "import pandas as pd \n",
    "from pyspark.ml.feature import StringIndexer # to convert class labels to numeric\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+-------+\n",
      "|text                                                                  |emotion|\n",
      "+----------------------------------------------------------------------+-------+\n",
      "|im feeling quite sad and sorry for myself but ill snap out of it soon |sadness|\n",
      "|i feel like i am still looking at a blank canvas blank pieces of paper|sadness|\n",
      "|i feel like a faithful servant                                        |love   |\n",
      "|i am just feeling cranky and blue                                     |anger  |\n",
      "+----------------------------------------------------------------------+-------+\n",
      "only showing top 4 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "file_path_testing =\"../data/test.txt\"\n",
    "file_path_training= \"../data/train.txt\"\n",
    "file_path_val= \"../data/val.txt\"\n",
    "\n",
    "\n",
    "def readFile(file_path):\n",
    "    df_row = spark.read.csv(file_path)\n",
    "    # print(\"original\")\n",
    "    # print(df_row.show(4,False))\n",
    "    df= df_row.withColumn(\"text\", split(df_row['_c0'], ';').getItem(0))\\\n",
    "            .withColumn('emotion', split(df_row['_c0'], ';').getItem(1))\n",
    "    df1= df.select(\"text\", \"emotion\")\n",
    "    return df1\n",
    "\n",
    "df_training = readFile(file_path= file_path_training)\n",
    "df_testing = readFile(file_path= file_path_training)\n",
    "df_val = readFile(file_path= file_path_val)\n",
    "print(df_val.show(4,False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size  (16000, 2) data points\n",
      "testing size  (16000, 2) data points\n",
      "validation size  (2000, 2) data points\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"training size \",df_training.toPandas().shape , \"data points\") \n",
    "print(\"testing size \",df_testing.toPandas().shape, \"data points\")\n",
    "print(\"validation size \",df_val.toPandas().shape, \"data points\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that we have unbalance data we have a lot of joy and sadness label and very little surprise and fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "| emotion|\n",
      "+--------+\n",
      "|     joy|\n",
      "|    love|\n",
      "|   anger|\n",
      "|    fear|\n",
      "|surprise|\n",
      "| sadness|\n",
      "+--------+\n",
      "\n",
      "training None\n",
      "+--------+-----+\n",
      "| emotion|count|\n",
      "+--------+-----+\n",
      "|     joy| 5362|\n",
      "|    love| 1304|\n",
      "|   anger| 2159|\n",
      "|    fear| 1937|\n",
      "|surprise|  572|\n",
      "| sadness| 4666|\n",
      "+--------+-----+\n",
      "\n",
      "training class count None\n"
     ]
    }
   ],
   "source": [
    "print(\"training\", df_training.select(\"emotion\").distinct().show())\n",
    "print(\"training class count\", df_training.groupBy(\"emotion\").count().show())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert labels to the numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+\n",
      "|                text|emotion|class|\n",
      "+--------------------+-------+-----+\n",
      "|i didnt feel humi...|sadness|    1|\n",
      "|i can go from fee...|sadness|    1|\n",
      "|im grabbing a min...|  anger|    2|\n",
      "|i am ever feeling...|   love|    4|\n",
      "|i am feeling grouchy|  anger|    2|\n",
      "+--------------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert sentiment to numbers  joy =0 love =1 \n",
    "#converting class labels\n",
    "string_indexer = StringIndexer(inputCol=\"emotion\",outputCol=\"class\")\n",
    "\n",
    "# fit the StringIndexer to the DataFrame and transform the DataFrame to add the 'fruits_numeric' column\n",
    "df_class = string_indexer.fit(df_training).transform(df_training)\\\n",
    ".withColumn('class', col('class').cast(\"integer\"))\n",
    "df_class.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|emotion |class|count|\n",
      "+--------+-----+-----+\n",
      "|sadness |1    |4666 |\n",
      "|love    |4    |1304 |\n",
      "|surprise|5    |572  |\n",
      "|joy     |0    |5362 |\n",
      "|fear    |3    |1937 |\n",
      "|anger   |2    |2159 |\n",
      "+--------+-----+-----+\n",
      "\n",
      "training class count None\n"
     ]
    }
   ],
   "source": [
    "# fit the StringIndexer to the DataFrame and transform the DataFrame to add the 'class_numeric' column\n",
    "df_class = string_indexer.fit(df_training).transform(df_training).withColumn('class', col('class').cast(\"integer\"))\n",
    "print(\"training class count\", df_class.groupBy(\"emotion\",\"class\").count()\n",
    ".show(truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|                text|emotion|\n",
      "+--------------------+-------+\n",
      "|i didnt feel humi...|sadness|\n",
      "|i can go from fee...|sadness|\n",
      "|im grabbing a min...|  anger|\n",
      "|i am ever feeling...|   love|\n",
      "|i am feeling grouchy|  anger|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize_udf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Apply the UDF to a DataFrame column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_training \u001b[39m=\u001b[39m df_training\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mtext_c\u001b[39m\u001b[39m\"\u001b[39m, lemmatize_udf(df_training[\u001b[39m\"\u001b[39m\u001b[39mtext_c\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatize_udf' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply the UDF to a DataFrame column\n",
    "df_training = df_training.withColumn(\"text_c\", lemmatize_udf(df_training[\"text_c\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| emotion|class|\n",
      "+--------+-----+\n",
      "| sadness|    1|\n",
      "|    love|    4|\n",
      "|surprise|    5|\n",
      "|     joy|    0|\n",
      "|    fear|    3|\n",
      "|   anger|    2|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "| emotion|class|\n",
      "+--------+-----+\n",
      "| sadness|    1|\n",
      "|    love|    4|\n",
      "|surprise|    5|\n",
      "|     joy|    0|\n",
      "|    fear|    3|\n",
      "|   anger|    2|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|emotion |class|\n",
      "+--------+-----+\n",
      "|sadness |1    |\n",
      "|love    |4    |\n",
      "|surprise|5    |\n",
      "|joy     |0    |\n",
      "|fear    |3    |\n",
      "|anger   |2    |\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the StringIndexer to the DataFrame and transform the DataFrame to add the 'class_numeric' column\n",
    "df_training_class = string_indexer.fit(df_training).transform(df_training).withColumn('class', col('class').cast(\"integer\"))\n",
    "df_testing_class = string_indexer.fit(df_testing).transform(df_testing).withColumn('class', col('class').cast(\"integer\"))\n",
    "df_val_class = string_indexer.fit(df_val).transform(df_val).withColumn('class', col('class').cast(\"integer\"))\n",
    "\n",
    "df_label1 = df_training_class.groupBy(\"emotion\",\"class\").count().select(\"emotion\", \"class\").show()\n",
    "df_label2 = df_testing_class.groupBy(\"emotion\",\"class\").count().select(\"emotion\", \"class\").show()\n",
    "df_label3 = df_val_class.groupBy(\"emotion\",\"class\").count().select(\"emotion\", \"class\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|emotion |class|\n",
      "+--------+-----+\n",
      "|sadness |1    |\n",
      "|love    |4    |\n",
      "|surprise|5    |\n",
      "|joy     |0    |\n",
      "|fear    |3    |\n",
      "|anger   |2    |\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_val_class.groupBy(\"emotion\",\"class\").count().select(\"emotion\", \"class\").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Text cleaning/preprocessing: This should be the first step as it involves removing any unnecessary or irrelevant information from the text, such as punctuation, numbers, special characters, and stop words. Text normalization techniques like lowercasing and stemming/lemmatization can also be applied during this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cleaning \n",
    "def clean(df):\n",
    "    # Remove html tags from text\n",
    "    df = df.withColumn(\"text_c\", F.regexp_replace(F.col(\"text\"), r'<[^>]+>', \"\"))\n",
    "    # Remove non-letters and remove punctuation \n",
    "    df = df.withColumn(\"text_c\", F.regexp_replace(\"text_c\", r\"[^a-zA-Z ]\", \"\"))\n",
    "    # Remove words 1, 2 char\n",
    "    df = df.withColumn(\"text_c\", F.regexp_replace(\"text_c\", r\"\\b\\w{1,2}\\b\", \"\"))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we clean all the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_clean = clean(df_training_class )\n",
    "df_testing_clean= clean(df_testing_class )\n",
    "df_val_clean =clean(df_val_class)\n",
    "\n",
    "training_size = df_training_clean.count()\n",
    "training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+--------------------+\n",
      "|                text| emotion|class|              text_c|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "|i didnt feel humi...| sadness|    1| didnt feel humil...|\n",
      "|i can go from fee...| sadness|    1| can  from feelin...|\n",
      "|im grabbing a min...|   anger|    2| grabbing  minute...|\n",
      "|i am ever feeling...|    love|    4|  ever feeling no...|\n",
      "|i am feeling grouchy|   anger|    2|     feeling grouchy|\n",
      "|ive been feeling ...| sadness|    1|ive been feeling ...|\n",
      "|ive been taking o...|surprise|    5|ive been taking  ...|\n",
      "|i feel as confuse...|    fear|    3| feel  confused a...|\n",
      "|i have been with ...|     joy|    0| have been with p...|\n",
      "| i feel romantic too|    love|    4|   feel romantic too|\n",
      "|i feel like i hav...| sadness|    1| feel like  have ...|\n",
      "|i do feel that ru...|     joy|    0|  feel that runni...|\n",
      "|i think it s the ...|   anger|    2| think   the easi...|\n",
      "|i feel low energy...| sadness|    1| feel low energy ...|\n",
      "|i have immense sy...|     joy|    0| have immense sym...|\n",
      "|i do not feel rea...|     joy|    0|  not feel reassu...|\n",
      "|i didnt really fe...| sadness|    1| didnt really fee...|\n",
      "|i feel pretty pat...| sadness|    1| feel pretty path...|\n",
      "|i started feeling...| sadness|    1| started feeling ...|\n",
      "|i now feel compro...|    fear|    3| now feel comprom...|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_training_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Define a UDF to apply the lemmatizer to a column\n",
    "lemmatize_udf = udf(lemmatize_text, StringType())\n",
    "# Apply the UDF to a DataFrame column\n",
    "df_training_clean_1 = df_training_clean.withColumn(\"text_c\", lemmatize_udf(df_training_clean[\"text_c\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for training data find out weights and create weight column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.335125 0.291625 0.1349375 0.1210625 0.0815 0.03575\n"
     ]
    }
   ],
   "source": [
    "w_joy = df_training_clean.filter('class == 0').count()/ training_size\n",
    "w_sadness = df_training_clean.filter('class == 1').count()/ training_size\n",
    "w_anger = df_training_clean.filter('class == 2').count()/ training_size\n",
    "w_fear = df_training_clean.filter('class == 3').count()/ training_size\n",
    "w_love = df_training_clean.filter('class == 4').count()/ training_size\n",
    "w_surprize = df_training_clean.filter('class == 5').count()/ training_size\n",
    "\n",
    "print(w_joy, w_sadness, w_anger, w_fear, w_love, w_surprize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+--------------------+---------+\n",
      "|                text| emotion|class|              text_c|   weight|\n",
      "+--------------------+--------+-----+--------------------+---------+\n",
      "|i didnt feel humi...| sadness|    1| didnt feel humil...| 0.291625|\n",
      "|i can go from fee...| sadness|    1| can  from feelin...| 0.291625|\n",
      "|im grabbing a min...|   anger|    2| grabbing  minute...|0.1349375|\n",
      "|i am ever feeling...|    love|    4|  ever feeling no...|   0.0815|\n",
      "|i am feeling grouchy|   anger|    2|     feeling grouchy|0.1349375|\n",
      "|ive been feeling ...| sadness|    1|ive been feeling ...| 0.291625|\n",
      "|ive been taking o...|surprise|    5|ive been taking  ...|  0.03575|\n",
      "|i feel as confuse...|    fear|    3| feel  confused a...|0.1210625|\n",
      "|i have been with ...|     joy|    0| have been with p...| 0.335125|\n",
      "| i feel romantic too|    love|    4|   feel romantic too|   0.0815|\n",
      "+--------------------+--------+-----+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_training_weight = df_training_clean.withColumn(\"weight\", when(F.col(\"class\")==0,w_joy).\n",
    "                                                  when(F.col(\"class\")==1,w_sadness).\n",
    "                                                  when(F.col(\"class\")==2,w_anger).\n",
    "                                                  when(F.col(\"class\")==3,w_fear).\n",
    "                                                  when(F.col(\"class\")==4,w_love).\n",
    "                                                  otherwise(w_surprize))\n",
    "\n",
    "df_training_weight.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the review text\n",
    "tokenizer = Tokenizer(inputCol=\"text_c\", outputCol=\"words\",)\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "# Create a count vectoriser\n",
    "countVectorizer = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=1000)\n",
    "# Calculate the TF-IDF\n",
    "idf = IDF(inputCol=countVectorizer.getOutputCol(), outputCol=\"featuresIDF\")\n",
    "# Crate a preprocessing pipeline with  4 stages\n",
    "pipeline_p = Pipeline(stages=[tokenizer,remover, countVectorizer, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_copyValues', '_copy_params', '_defaultParamMap', '_dummy', '_from_java', '_is_protocol', '_paramMap', '_params', '_randomUID', '_resetUid', '_resolveParam', '_set', '_setDefault', '_shouldOwn', '_testOwnParam', '_to_java', '_transform', 'clear', 'copy', 'explainParam', 'explainParams', 'extractParamMap', 'getOrDefault', 'getParam', 'hasDefault', 'hasParam', 'isDefined', 'isSet', 'load', 'params', 'read', 'save', 'set', 'stages', 'transform', 'uid', 'write']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Learn the data preprocessing model\n",
    "data_model  = pipeline_p.fit(df_training_weight)\n",
    "print(dir(data_model ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "transformed_training= data_model.transform(df_training_weight)\n",
    "transformed_test= data_model.transform(df_testing_clean)\n",
    "transformed_val= data_model.transform(df_val_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0.0, 'feel': 0.41, 'feeling': 1.17, 'like': 1.79, 'really': 2.91, 'know': 2.98, 'time': 3.06, 'get': 3.14, 'little': 3.13, 'people': 3.26}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data_model.stages[2].vocabulary[:10]\n",
    "data_model.stages\n",
    "\n",
    "# Get the vocabulary of the CountVectorizerModel\n",
    "vocab = data_model.stages[2].vocabulary[:10]\n",
    "\n",
    "# Get the IDF values from the IDFModel\n",
    "idf = data_model.stages[3].idf[:10]\n",
    "# Create a dictionary to store the word IDF rates\n",
    "word_idf = {}\n",
    "\n",
    "for item in range(10):\n",
    "    word =vocab[item]\n",
    "    idf_rate = float(idf[item])\n",
    "    word_idf[word] = round(idf_rate,2)\n",
    "\n",
    "print(word_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAo2ElEQVR4nO3de1xVdb7/8TegbkUB74BI4IyKeEdLAyfFc1R0zJE643isGdRRs/OQI96LptE5Wg+aHEMbHc3pCJ5KLTV1SkfHNDSvecPUvGdCBZiTgmChA9/fH/3c0x5B2Qh+BV/Px2M9Hu2119rrszeir9ZesD2MMUYAAACWeNoeAAAA3N+IEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhVw/YAZVFcXKyvvvpKPj4+8vDwsD0OAAAoA2OMrly5ombNmsnTs/TzH1UiRr766isFBwfbHgMAAJRDZmammjdvXur9VSJGfHx8JH3/ZHx9fS1PAwAAyiIvL0/BwcHOf8dLUyVi5MZbM76+vsQIAABVzO0useACVgAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMCqGrYHAADgfhL67HrbI9zk85cGWj0+Z0YAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCo+KA8A7nN8cBtsI0YAAFUSEVV98DYNAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArOL3jABABeH3XgDlw5kRAABglVsxsnDhQnXs2FG+vr7y9fVVZGSk/vrXv95yn5UrV6pNmzaqXbu2OnTooA0bNtzRwAAAoHpxK0aaN2+ul156SQcOHND+/fv1b//2bxo8eLCOHTtW4va7du3SsGHDNGrUKB06dEixsbGKjY3V0aNHK2R4AABQ9bkVI4MGDdJPf/pTtWrVSq1bt9aLL76oevXqac+ePSVuP2/ePPXv319Tp05VeHi4Zs2apS5dumj+/PkVMjwAAKj6yn3NSFFRkVasWKGCggJFRkaWuM3u3bvVp08fl3UxMTHavXv3LR+7sLBQeXl5LgsAAKie3I6RI0eOqF69enI4HHr66ae1Zs0atW3btsRts7Oz5e/v77LO399f2dnZtzxGUlKS/Pz8nEtwcLC7YwIAgCrC7RgJCwtTenq69u7dq//6r//S8OHD9emnn1boUImJicrNzXUumZmZFfr4AADg3uH27xmpVauWWrZsKUnq2rWr9u3bp3nz5um11167aduAgADl5OS4rMvJyVFAQMAtj+FwOORwONwdDQAAVEF3/EvPiouLVVhYWOJ9kZGR2rJliyZMmOBct3nz5lKvMQEAiV8eBtxv3IqRxMREDRgwQA888ICuXLmiZcuWKS0tTZs2bZIkxcXFKSgoSElJSZKkhIQE9erVS3PmzNHAgQO1YsUK7d+/X4sXL674ZwIAAKokt2LkwoULiouLU1ZWlvz8/NSxY0dt2rRJffv2lSRlZGTI0/Ofl6FERUVp2bJlev755/Xcc8+pVatWWrt2rdq3b1+xzwIAAFRZbsXI//7v/97y/rS0tJvWDRkyREOGDHFrKAAAcP/gs2kAAIBVxAgAALCKGAEAAFbd8Y/2Arh38SOyAKoCzowAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKrdiJCkpSQ899JB8fHzUtGlTxcbG6uTJk7fcJzU1VR4eHi5L7dq172hoAABQfbgVI9u2bdO4ceO0Z88ebd68WdevX1e/fv1UUFBwy/18fX2VlZXlXM6fP39HQwMAgOqjhjsbb9y40eV2amqqmjZtqgMHDqhnz56l7ufh4aGAgIDyTQgAAKq1O7pmJDc3V5LUsGHDW26Xn5+vkJAQBQcHa/DgwTp27Ngtty8sLFReXp7LAgAAqqdyx0hxcbEmTJigHj16qH379qVuFxYWpiVLlmjdunV68803VVxcrKioKH3xxRel7pOUlCQ/Pz/nEhwcXN4xAQDAPa7cMTJu3DgdPXpUK1asuOV2kZGRiouLU+fOndWrVy+9++67atKkiV577bVS90lMTFRubq5zyczMLO+YAADgHufWNSM3xMfH6/3339f27dvVvHlzt/atWbOmIiIidObMmVK3cTgccjgc5RkNAABUMW6dGTHGKD4+XmvWrNHWrVvVokULtw9YVFSkI0eOKDAw0O19AQBA9ePWmZFx48Zp2bJlWrdunXx8fJSdnS1J8vPzU506dSRJcXFxCgoKUlJSkiRp5syZevjhh9WyZUtdvnxZs2fP1vnz5zV69OgKfioAAKAqcitGFi5cKEmKjo52WZ+SkqIRI0ZIkjIyMuTp+c8TLpcuXdKYMWOUnZ2tBg0aqGvXrtq1a5fatm17Z5MDAIBqwa0YMcbcdpu0tDSX28nJyUpOTnZrKAAAcP/gs2kAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFa5FSNJSUl66KGH5OPjo6ZNmyo2NlYnT5687X4rV65UmzZtVLt2bXXo0EEbNmwo98AAAKB6cStGtm3bpnHjxmnPnj3avHmzrl+/rn79+qmgoKDUfXbt2qVhw4Zp1KhROnTokGJjYxUbG6ujR4/e8fAAAKDqq+HOxhs3bnS5nZqaqqZNm+rAgQPq2bNnifvMmzdP/fv319SpUyVJs2bN0ubNmzV//nwtWrSonGMDAIDq4o6uGcnNzZUkNWzYsNRtdu/erT59+risi4mJ0e7du0vdp7CwUHl5eS4LAAConsodI8XFxZowYYJ69Oih9u3bl7pddna2/P39Xdb5+/srOzu71H2SkpLk5+fnXIKDg8s7JgAAuMeVO0bGjRuno0ePasWKFRU5jyQpMTFRubm5ziUzM7PCjwEAAO4Nbl0zckN8fLzef/99bd++Xc2bN7/ltgEBAcrJyXFZl5OTo4CAgFL3cTgccjgc5RkNAABUMW6dGTHGKD4+XmvWrNHWrVvVokWL2+4TGRmpLVu2uKzbvHmzIiMj3ZsUAABUS26dGRk3bpyWLVumdevWycfHx3ndh5+fn+rUqSNJiouLU1BQkJKSkiRJCQkJ6tWrl+bMmaOBAwdqxYoV2r9/vxYvXlzBTwUAAFRFbp0ZWbhwoXJzcxUdHa3AwEDn8vbbbzu3ycjIUFZWlvN2VFSUli1bpsWLF6tTp05atWqV1q5de8uLXgEAwP3DrTMjxpjbbpOWlnbTuiFDhmjIkCHuHAoAANwn+GwaAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq2rYHgCoCkKfXW97hJt8/tJA2yMAQIXgzAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFa5HSPbt2/XoEGD1KxZM3l4eGjt2rW33D4tLU0eHh43LdnZ2eWdGQAAVCNux0hBQYE6deqkBQsWuLXfyZMnlZWV5VyaNm3q7qEBAEA1VMPdHQYMGKABAwa4faCmTZuqfv36bu8HAACqt7t2zUjnzp0VGBiovn37aufOnbfctrCwUHl5eS4LAAConio9RgIDA7Vo0SKtXr1aq1evVnBwsKKjo3Xw4MFS90lKSpKfn59zCQ4OruwxAQCAJW6/TeOusLAwhYWFOW9HRUXp7NmzSk5O1htvvFHiPomJiZo0aZLzdl5eHkECAEA1VekxUpJu3bppx44dpd7vcDjkcDju4kQAAMAWK79nJD09XYGBgTYODQAA7jFunxnJz8/XmTNnnLfPnTun9PR0NWzYUA888IASExP15Zdf6v/+7/8kSXPnzlWLFi3Url07fffdd3r99de1detW/e1vf6u4ZwEAAKost2Nk//796t27t/P2jWs7hg8frtTUVGVlZSkjI8N5/7Vr1zR58mR9+eWX8vb2VseOHfXBBx+4PAYAALh/uR0j0dHRMsaUen9qaqrL7WnTpmnatGluDwYAAO4PfDYNAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKtq2B4A95fQZ9fbHuEmn7800PYIAHBf48wIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKvcjpHt27dr0KBBatasmTw8PLR27drb7pOWlqYuXbrI4XCoZcuWSk1NLceoAACgOnI7RgoKCtSpUyctWLCgTNufO3dOAwcOVO/evZWenq4JEyZo9OjR2rRpk9vDAgCA6qeGuzsMGDBAAwYMKPP2ixYtUosWLTRnzhxJUnh4uHbs2KHk5GTFxMS4e3gAAFDNVPo1I7t371afPn1c1sXExGj37t2l7lNYWKi8vDyXBQAAVE+VHiPZ2dny9/d3Wefv76+8vDx9++23Je6TlJQkPz8/5xIcHFzZYwIAAEvuyZ+mSUxMVG5urnPJzMy0PRIAAKgkbl8z4q6AgADl5OS4rMvJyZGvr6/q1KlT4j4Oh0MOh6OyRwMAAPeASj8zEhkZqS1btris27x5syIjIyv70AAAoApwO0by8/OVnp6u9PR0Sd//6G56eroyMjIkff8WS1xcnHP7p59+Wp999pmmTZumEydO6E9/+pPeeecdTZw4sWKeAQAAqNLcjpH9+/crIiJCERERkqRJkyYpIiJC06dPlyRlZWU5w0SSWrRoofXr12vz5s3q1KmT5syZo9dff50f6wUAAJLKcc1IdHS0jDGl3l/Sb1eNjo7WoUOH3D0UAAC4D9yTP00DAADuH8QIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWlStGFixYoNDQUNWuXVvdu3fXxx9/XOq2qamp8vDwcFlq165d7oEBAED14naMvP3225o0aZJmzJihgwcPqlOnToqJidGFCxdK3cfX11dZWVnO5fz583c0NAAAqD7cjpFXXnlFY8aM0ciRI9W2bVstWrRI3t7eWrJkSan7eHh4KCAgwLn4+/vf0dAAAKD6cCtGrl27pgMHDqhPnz7/fABPT/Xp00e7d+8udb/8/HyFhIQoODhYgwcP1rFjx255nMLCQuXl5bksAACgenIrRi5evKiioqKbzmz4+/srOzu7xH3CwsK0ZMkSrVu3Tm+++aaKi4sVFRWlL774otTjJCUlyc/Pz7kEBwe7MyYAAKhCKv2naSIjIxUXF6fOnTurV69eevfdd9WkSRO99tprpe6TmJio3Nxc55KZmVnZYwIAAEtquLNx48aN5eXlpZycHJf1OTk5CggIKNNj1KxZUxERETpz5kyp2zgcDjkcDndGAwAAVZRbZ0Zq1aqlrl27asuWLc51xcXF2rJliyIjI8v0GEVFRTpy5IgCAwPdmxQAAFRLbp0ZkaRJkyZp+PDhevDBB9WtWzfNnTtXBQUFGjlypCQpLi5OQUFBSkpKkiTNnDlTDz/8sFq2bKnLly9r9uzZOn/+vEaPHl2xzwQAAFRJbsfI0KFD9fXXX2v69OnKzs5W586dtXHjRudFrRkZGfL0/OcJl0uXLmnMmDHKzs5WgwYN1LVrV+3atUtt27atuGcBAACqLLdjRJLi4+MVHx9f4n1paWkut5OTk5WcnFyewwAAgPsAn00DAACsIkYAAIBVxAgAALCqXNeMwL7QZ9fbHuEmn7800PYIAIAqiDMjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsKpcMbJgwQKFhoaqdu3a6t69uz7++ONbbr9y5Uq1adNGtWvXVocOHbRhw4ZyDQsAAKoft2Pk7bff1qRJkzRjxgwdPHhQnTp1UkxMjC5cuFDi9rt27dKwYcM0atQoHTp0SLGxsYqNjdXRo0fveHgAAFD1uR0jr7zyisaMGaORI0eqbdu2WrRokby9vbVkyZISt583b5769++vqVOnKjw8XLNmzVKXLl00f/78Ox4eAABUfTXc2fjatWs6cOCAEhMTnes8PT3Vp08f7d69u8R9du/erUmTJrmsi4mJ0dq1a0s9TmFhoQoLC523c3NzJUl5eXnujFutFRdetT3CTcry9WHuisPcdxdz313MfXdV1r+vNx7XGHPrDY0bvvzySyPJ7Nq1y2X91KlTTbdu3Urcp2bNmmbZsmUu6xYsWGCaNm1a6nFmzJhhJLGwsLCwsLBUgyUzM/OWfeHWmZG7JTEx0eVsSnFxsb755hs1atRIHh4eFicrXV5enoKDg5WZmSlfX1/b45QZc99dzH13Mffdxdx3V1WY2xijK1euqFmzZrfczq0Yady4sby8vJSTk+OyPicnRwEBASXuExAQ4Nb2kuRwOORwOFzW1a9f351RrfH19b1n/1DcCnPfXcx9dzH33cXcd9e9Prefn99tt3HrAtZatWqpa9eu2rJli3NdcXGxtmzZosjIyBL3iYyMdNlekjZv3lzq9gAA4P7i9ts0kyZN0vDhw/Xggw+qW7dumjt3rgoKCjRy5EhJUlxcnIKCgpSUlCRJSkhIUK9evTRnzhwNHDhQK1as0P79+7V48eKKfSYAAKBKcjtGhg4dqq+//lrTp09Xdna2OnfurI0bN8rf31+SlJGRIU/Pf55wiYqK0rJly/T888/rueeeU6tWrbR27Vq1b9++4p7FPcDhcGjGjBk3vb10r2Puu4u57y7mvruY++6qqnOXxMOY2/28DQAAQOXhs2kAAIBVxAgAALCKGAEAAFYRI9WQMUZPPfWUGjZsKA8PD6Wnp9/xY0ZHR2vChAn35CyhoaGaO3fuHT/unfjhTP86j4eHxy0//qAq+vzzz12+nmlpafLw8NDly5cr9Djl/XN3r6qs1wnfu9X3oTuq4/esdG8/r3vyN7DizmzcuFGpqalKS0vTj370IzVu3Lhaz7Jv3z7VrVu3wh+3vO61eWBPdHS0Onfu7PxHMSoqSllZWWX6JVDVTWpqqiZMmHDXQuxfvw89PDy0Zs0axcbGOtf97ne/09q1ayvkf5JwZ4iRaujs2bMKDAxUVFSU7VHuyixNmjSptMcuj3ttnn917do11apVy/YY96VatWrd8rdPo+Lc69+HcMXbNNXMiBEj9N///d/KyMiQh4eHQkNDVVxcrKSkJLVo0UJ16tRRp06dtGrVKpf9jh49qgEDBqhevXry9/fXr371K128eLFKzFLS2yKvv/66HnvsMXl7e6tVq1b6y1/+4rLPX/7yF7Vq1Uq1a9dW7969tXTp0go7fX6708MzZsxQYGCgPvnkE0nSjh079Mgjj6hOnToKDg7W+PHjVVBQcMdz3BAdHa34+HhNmDBBjRs3VkxMzG1f440bN+onP/mJ6tevr0aNGunRRx/V2bNny3S8goIC+fr63vR1Xbt2rerWrasrV66U+7msX79efn5+euuttzRixAjFxsbqD3/4gwIDA9WoUSONGzdO169fd25/6dIlxcXFqUGDBvL29taAAQN0+vRpSd+/hdikSROXOTt37qzAwEDn7R07dsjhcOjqVfc/ZXXEiBHatm2b5s2bJw8PD3l4eCg1NdXlz1lqaqrq16+v999/X2FhYfL29tbPf/5zXb16VUuXLlVoaKgaNGig8ePHq6ioyPnYhYWFmjJlioKCglS3bl11795daWlpbs/ojitXrujJJ59U3bp1FRgYqOTkZJe3RW41U1pamkaOHKnc3Fzna/G73/2uUuf94fdhaGioJOmxxx5z/l2Umpqq//mf/9Hhw4ddvj4lyczM1C9+8QvVr19fDRs21ODBg/X555+XeZYb34Px8fHy8/NT48aN9dvf/tb5SbZl+XquXr1a7dq1k8PhUGhoqObMmXPT8501a5aGDRumunXrKigoSAsWLLjlXHf6vCrU7T6pF1XL5cuXzcyZM03z5s1NVlaWuXDhgnnhhRdMmzZtzMaNG83Zs2dNSkqKcTgcJi0tzRhjzKVLl0yTJk1MYmKiOX78uDl48KDp27ev6d27t/Nxe/XqZRISEu7JWUJCQkxycrLztiTTvHlzs2zZMnP69Gkzfvx4U69ePfP3v//dGGPMZ599ZmrWrGmmTJliTpw4YZYvX26CgoKMJHPp0iX3XvASZippnjVr1pji4mITHx9vQkNDzenTp40xxpw5c8bUrVvXJCcnm1OnTpmdO3eaiIgIM2LEiHLNUdps9erVM1OnTjUnTpwwe/bsue1rvGrVKrN69Wpz+vRpc+jQITNo0CDToUMHU1RUZIwx5ty5c0aSOXTokDHGmA8//NDl9RszZoz56U9/6jLHz372MxMXF+f27Dde17feesv4+PiY9957zxhjzPDhw42vr695+umnzfHjx817771nvL29zeLFi12OGR4ebrZv327S09NNTEyMadmypbl27ZoxxpjHH3/cjBs3zhhjzDfffGNq1apl/Pz8zPHjx40xxrzwwgumR48ebs18w+XLl01kZKQZM2aMycrKMllZWeaDDz5weZ1SUlJMzZo1Td++fc3BgwfNtm3bTKNGjUy/fv3ML37xC3Ps2DHz3nvvmVq1apkVK1Y4H3v06NEmKirKbN++3Zw5c8bMnj3bOBwOc+rUqXLNWhajR482ISEh5oMPPjBHjhwxjz32mPHx8XF+fW41U2FhoZk7d67x9fV1vhZXrlyp8BlL+z68cOGCkWRSUlKcfxddvXrVTJ482bRr184509WrV40x//yeNcaYa9eumfDwcPPrX//afPLJJ+bTTz81TzzxhAkLCzOFhYVlnqtevXomISHBnDhxwrz55psuf1Zv9/Xcv3+/8fT0NDNnzjQnT540KSkppk6dOiYlJcV5jJCQEOPj42OSkpLMyZMnzauvvmq8vLzM3/72N+c2Ff28KhIxUg0lJyebkJAQY4wx3333nfH29ja7du1y2WbUqFFm2LBhxhhjZs2aZfr16+dyf2ZmppFkTp48aYwpX4zcrVlK+sf/+eefd97Oz883ksxf//pXY4wxzzzzjGnfvr3LMX7zm99UaoysXLnSPPHEEyY8PNx88cUXLs/9qaeecnmsjz76yHh6eppvv/22XLOUNFtERITzdlle43/19ddfG0nmyJEjxpjbx8jevXuNl5eX+eqrr4wxxuTk5JgaNWo4o9Od2RMSEsz8+fONn5+fy/7Dhw83ISEh5h//+Idz3ZAhQ8zQoUONMcacOnXKSDI7d+503n/x4kVTp04d88477xhjjHn11VdNu3btjDHGrF271nTv3t0MHjzYLFy40BhjTJ8+fcxzzz3n1swlzX/Dv75OKSkpRpI5c+aMc5uxY8cab29vl3+sY2JizNixY40xxpw/f954eXmZL7/80uVY//7v/24SExPLPeut5OXlmZo1a5qVK1c6112+fNl4e3ubhISEMs2UkpJi/Pz8KmW+G8ryPwU/NGPGDNOpU6ebHueH277xxhsmLCzMFBcXO+8vLCw0derUMZs2bSrzXOHh4S6P8cwzz5jw8PAyvXZPPPGE6du3r8v9U6dONW3btnXeDgkJMf3793fZZujQoWbAgAGV9rwqEteMVHNnzpzR1atX1bdvX5f1165dU0REhCTp8OHD+vDDD1WvXr2b9j979qxat25d5Wbp2LGj87/r1q0rX19fXbhwQZJ08uRJPfTQQy7bd+vWza3n4q6JEyfK4XBoz549LhfxHj58WJ988oneeust5zpjjIqLi3Xu3DmFh4dXyPG7du3qcszbvcanT5/W9OnTtXfvXl28eFHFxcWSvv+4h7J8lEO3bt3Url07LV26VM8++6zefPNNhYSEqGfPnm7PvmrVKl24cEE7d+686evWrl07eXl5OW8HBgbqyJEjkqTjx4+rRo0a6t69u/P+Ro0aKSwsTMePH5ck9erVSwkJCfr666+1bds2RUdHKyAgQGlpaRo1apR27dqladOmuT2zO7y9vfXjH//Yedvf31+hoaEuXx9/f3/nn98jR46oqKjopu+FwsJCNWrUqFJm/Oyzz3T9+nWX7xM/Pz+FhYVZm+luOXz4sM6cOSMfHx+X9d99912Z37qUpIcfflgeHh7O25GRkZozZ06ZXrvjx49r8ODBLvf36NFDc+fOVVFRkfN74F8/gDYyMrLUt4wr6nlVFGKkmsvPz5f0/XvtQUFBLvfd+DyD/Px8DRo0SL///e9v2v+H759XpVlq1qzpctvDw8P5D6oNffv21fLly7Vp0yY9+eSTzvX5+fkaO3asxo8ff9M+DzzwQIUd/4c/VVCW13jQoEEKCQnRn//8ZzVr1kzFxcVq3769rl27VuZjjh49WgsWLNCzzz6rlJQUjRw50uUv47KKiIjQwYMHtWTJEj344IMuj3GnX+cOHTqoYcOG2rZtm7Zt26YXX3xRAQEB+v3vf699+/bp+vXrlX4heEnP4VbPKz8/X15eXjpw4IBLiEkqMTDvhntxpoqSn5+vrl27uvwPww0VcZGsrdeusp+Xu4iRaq5t27ZyOBzKyMhQr169StymS5cuWr16tUJDQ1WjRuX9kbhXZgkLC9OGDRtc1u3bt69SjnXDz372Mw0aNEhPPPGEvLy89J//+Z+Svn++n376qVq2bFmpx/+h273Gf//733Xy5En9+c9/1iOPPCLp+ws53fXLX/5S06ZN06uvvqpPP/1Uw4cPL9e8P/7xjzVnzhxFR0fLy8tL8+fPL9N+4eHh+sc//qG9e/c6g+LGc2vbtq2k7/+Rf+SRR7Ru3TodO3ZMP/nJT+Tt7a3CwkK99tprevDBB+/ox7Rr1arlcuFpRYiIiFBRUZEuXLjg/PpUth/96EeqWbOm9u3b54zk3NxcnTp1Sj179izTTJXxWrijZs2aNx2/LDN16dJFb7/9tpo2bSpfX99yH3/v3r0ut/fs2aNWrVqV6bULDw/Xzp07Xdbt3LlTrVu3dgmYPXv23HSM0s6uVtTzqij8NE015+PjoylTpmjixIlaunSpzp49q4MHD+qPf/yjli5dKkkaN26cvvnmGw0bNkz79u3T2bNntWnTJo0cObJC//K4V2YZO3asTpw4oWeeeUanTp3SO++847yKvjz/515Wjz32mN544w2NHDnS+RMczzzzjHbt2qX4+Hilp6fr9OnTWrduneLj4yttjtu9xg0aNFCjRo20ePFinTlzRlu3btWkSZPcPk6DBg30+OOPa+rUqerXr5+aN29e7plbt26tDz/8UKtXry7zL0Fr1aqVBg8erDFjxmjHjh06fPiwfvnLXyooKMjllHd0dLSWL1+uzp07q169evL09FTPnj311ltvlRrNZRUaGqq9e/fq888/d3m76060bt1aTz75pOLi4vTuu+/q3Llz+vjjj5WUlKT169ff8eOXxMfHR8OHD9fUqVP14Ycf6tixYxo1apQ8PT3l4eFRpplCQ0OVn5+vLVu26OLFi+X6CaU7ERoaqi1btig7O1uXLl1yrjt37pzS09N18eJFFRYW3rTfk08+qcaNG2vw4MH66KOPdO7cOaWlpWn8+PH64osvynz8jIwMTZo0SSdPntTy5cv1xz/+UQkJCWV67SZPnqwtW7Zo1qxZOnXqlJYuXar58+drypQpLsfYuXOnXn75ZZ06dUoLFizQypUrlZCQUOI8FfW8Kgoxch+YNWuWfvvb3yopKUnh4eHq37+/1q9frxYtWkiSmjVrpp07d6qoqEj9+vVThw4dNGHCBNWvX1+enhX7R+RemKVFixZatWqV3n33XXXs2FELFy7Ub37zG0mq9I/i/vnPf66lS5fqV7/6lfP427Zt06lTp/TII48oIiJC06dPV7NmzSpthtu9xp6enlqxYoUOHDig9u3ba+LEiZo9e3a5jjVq1Chdu3ZNv/71r+947rCwMG3dulXLly/X5MmTy7RPSkqKunbtqkcffVSRkZEyxmjDhg0ub4P06tVLRUVFio6Odq6Ljo6+aV15TJkyRV5eXmrbtq2aNGmijIyMO3q8G1JSUhQXF6fJkycrLCxMsbGxLmctKsMrr7yiyMhIPfroo+rTp4969Oih8PBw1a5du0wzRUVF6emnn9bQoUPVpEkTvfzyy5U2a0nmzJmjzZs3Kzg42HmN2n/8x3+of//+6t27t5o0aaLly5fftJ+3t7e2b9+uBx54QI8//rjCw8M1atQofffdd26dUYiLi9O3336rbt26ady4cUpISNBTTz0l6favXZcuXfTOO+9oxYoVat++vaZPn66ZM2dqxIgRLseYPHmy9u/fr4iICL3wwgt65ZVXFBMTU+I8FfW8KorH/7/CFrivvfjii1q0aJEyMzNtj1KtvPHGG5o4caK++uorftFaNVNQUKCgoCDNmTNHo0aNsj3OPe1ffxNvZQgNDdWECROq7McncM0I7kt/+tOf9NBDD6lRo0bauXOnZs+eXalvjdxvrl69qqysLL300ksaO3YsIVINHDp0SCdOnFC3bt2Um5urmTNnStJNP+UBlAdv0+C+dPr0aQ0ePFht27bVrFmzNHny5Er/jZD3k5dffllt2rRRQECAEhMTbY+DCvKHP/xBnTp1Up8+fVRQUKCPPvrI6mdfofrgbRoAAGAVZ0YAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBV/w9gfgtfb5fQAwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "words= list(word_idf.keys())\n",
    "values = list(word_idf.values())\n",
    "\n",
    "plt.bar(range(len(word_idf)), values, tick_label= words)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|emotion|class|              text_c|   weight|               words|            filtered|         rawFeatures|         featuresIDF|\n",
      "+--------------------+-------+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|i didnt feel humi...|sadness|    1| didnt feel humil...| 0.291625|[, didnt, feel, h...|[, didnt, feel, h...|(1000,[0,1,46,565...|(1000,[0,1,46,565...|\n",
      "|i can go from fee...|sadness|    1| can  from feelin...| 0.291625|[, can, , from, f...|[, , feeling, , h...|(1000,[0,2,40,51,...|(1000,[0,2,40,51,...|\n",
      "|im grabbing a min...|  anger|    2| grabbing  minute...|0.1349375|[, grabbing, , mi...|[, grabbing, , mi...|(1000,[0,1,173,32...|(1000,[0,1,173,32...|\n",
      "|i am ever feeling...|   love|    4|  ever feeling no...|   0.0815|[, , ever, feelin...|[, , ever, feelin...|(1000,[0,2,5,13,6...|(1000,[0,2,5,13,6...|\n",
      "|i am feeling grouchy|  anger|    2|     feeling grouchy|0.1349375|[, , feeling, gro...|[, , feeling, gro...|(1000,[0,2,955],[...|(1000,[0,2,955],[...|\n",
      "+--------------------+-------+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|emotion|class|              text_c|               words|            filtered|         rawFeatures|         featuresIDF|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|i didnt feel humi...|sadness|    1| didnt feel humil...|[, didnt, feel, h...|[, didnt, feel, h...|(1000,[0,1,46,565...|(1000,[0,1,46,565...|\n",
      "|i can go from fee...|sadness|    1| can  from feelin...|[, can, , from, f...|[, , feeling, , h...|(1000,[0,2,40,51,...|(1000,[0,2,40,51,...|\n",
      "|im grabbing a min...|  anger|    2| grabbing  minute...|[, grabbing, , mi...|[, grabbing, , mi...|(1000,[0,1,173,32...|(1000,[0,1,173,32...|\n",
      "|i am ever feeling...|   love|    4|  ever feeling no...|[, , ever, feelin...|[, , ever, feelin...|(1000,[0,2,5,13,6...|(1000,[0,2,5,13,6...|\n",
      "|i am feeling grouchy|  anger|    2|     feeling grouchy|[, , feeling, gro...|[, , feeling, gro...|(1000,[0,2,955],[...|(1000,[0,2,955],[...|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_test.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------ Adding Feature selection method -----------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChiSquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "tokenizer = Tokenizer(inputCol=\"text_c\", outputCol=\"words\",)\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "# countVectorizer = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=1000)\n",
    "# Define n-gram model\n",
    "ngram = NGram(n=2, inputCol=remover.getOutputCol(),  outputCol=\"ngrams\")\n",
    "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=countVectorizer.getOutputCol(), outputCol=\"featuresIDF\")\n",
    "selector = ChiSqSelector(numTopFeatures=200, featuresCol=idf.getOutputCol(), outputCol=\"features\", labelCol=\"class\")\n",
    "# Crate a preprocessing pipeline wiht 5 stages\n",
    "pipeline_2 = Pipeline(stages=[tokenizer,remover,ngram, hashingTF, idf, selector])\n",
    "# Learn the data preprocessing model\n",
    "data_model_2 = pipeline_2.fit(df_training_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sweet', 'stressed', 'strange', 'overwhelmed', 'weird', 'loved', 'angry', 'amazing', 'blessed', 'passionate', 'helpless', 'accepted', 'afraid', 'hated', 'agitated', 'cold', 'towards', 'anxious', 'supporting', 'nervous', 'glad', 'generous', 'sorry', 'loving', 'depressed', 'safe', 'frustrated', 'scared', 'guilty', 'confident', 'beloved', 'miserable', 'exhausted', 'hot', 'missed', 'terrified', 'lovely', 'proud', 'ashamed', 'tortured', 'irritable', 'liked', 'satisfied', 'mad', 'awkward', 'selfish', 'content', 'successful', 'caring', 'lethargic', 'amazed', 'unsure', 'confused', 'bothered', 'unhappy', 'funny', 'inspired', 'popular', 'embarrassed', 'vulnerable', 'shitty', 'annoyed', 'brave', 'talented', 'rich', 'hopeless', 'uncertain', 'greedy', 'uncomfortable', 'shaken', 'punished', 'surprised', 'fucked', 'irritated', 'insecure', 'melancholy', 'devastated', 'impressed', 'aching', 'discouraged', 'offended', 'jealous', 'useful', 'resentful', 'rejected', 'burdened', 'violent', 'dull', 'homesick', 'pleasant', 'divine', 'reluctant', 'isolated', 'respected', 'useless', 'hopeful', 'cranky', 'paranoid', 'determined', 'curious', 'beaten', 'rushed', 'rude', 'insulted', 'assured', 'apprehensive', 'relaxed', 'pissed', 'pressured', 'honored', 'ugly', 'groggy', 'gloomy', 'wronged', 'numb', 'unwelcome', 'dangerous', 'valued', 'shy', 'pathetic', 'sympathetic', 'friendly', 'faithful', 'dumb', 'heartbroken', 'intimidated', 'ignored', 'dissatisfied', 'worthless', 'sincere', 'disheartened', 'longing', 'superior', 'bitter', 'threatened', 'hesitant', 'regretful', 'lousy', 'festive', 'disturbed', 'vain', 'broke', 'naughty', 'clever', 'bitchy', 'supportive', 'shocked', 'nostalgic', 'romantic', 'damaged', 'troubled', 'inadequate', 'drained', 'distracted', 'humiliated', 'fearful', 'disgusted', 'shaky', 'defeated', 'energetic', 'petty', 'listless', 'rotten', 'welcomed', 'unimportant', 'gentle', 'deprived', 'unfortunate', 'needy', 'doomed', 'impatient', 'rebellious', 'horny', 'worthwhile', 'loyal', 'tender', 'toward', 'virtuous', 'restless', 'jaded', 'invigorated', 'frightened', 'contented', 'grumpy', 'abused', 'delicate', 'stubborn', 'fond', 'frantic', 'skeptical', 'envious', 'dazed', 'doubtful', 'distressed', 'hostile', 'suspicious', 'hateful', 'indecisive', 'grouchy', 'distraught']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_v2 = data_model_2.stages[2].vocabulary\n",
    "selected_indexes = data_model_2.stages[4].selectedFeatures\n",
    "selected_words = [vocabulary_v2[i] for i in selected_indexes]\n",
    "print(selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "transformed_training_p2= data_model.transform(df_training_weight)\n",
    "transformed_test_p2 = data_model.transform(df_testing_clean)\n",
    "transformed_val_p2 = data_model.transform(df_val_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_3733c725bad1,\n",
       " StopWordsRemover_1833830c8b29,\n",
       " CountVectorizerModel: uid=CountVectorizer_ea3f4a758a33, vocabularySize=1000,\n",
       " IDFModel: uid=IDF_d51a5a276699, numDocs=16000, numFeatures=1000]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|emotion|class|              text_c|   weight|               words|            filtered|         rawFeatures|         featuresIDF|\n",
      "+--------------------+-------+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|i didnt feel humi...|sadness|    1| didnt feel humil...| 0.291625|[, didnt, feel, h...|[, didnt, feel, h...|(1000,[0,1,46,565...|(1000,[0,1,46,565...|\n",
      "|i can go from fee...|sadness|    1| can  from feelin...| 0.291625|[, can, , from, f...|[, , feeling, , h...|(1000,[0,2,40,51,...|(1000,[0,2,40,51,...|\n",
      "|im grabbing a min...|  anger|    2| grabbing  minute...|0.1349375|[, grabbing, , mi...|[, grabbing, , mi...|(1000,[0,1,173,32...|(1000,[0,1,173,32...|\n",
      "|i am ever feeling...|   love|    4|  ever feeling no...|   0.0815|[, , ever, feelin...|[, , ever, feelin...|(1000,[0,2,5,13,6...|(1000,[0,2,5,13,6...|\n",
      "|i am feeling grouchy|  anger|    2|     feeling grouchy|0.1349375|[, , feeling, gro...|[, , feeling, gro...|(1000,[0,2,955],[...|(1000,[0,2,955],[...|\n",
      "+--------------------+-------+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_training_p2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|emotion|class|              text_c|               words|            filtered|         rawFeatures|         featuresIDF|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|i didnt feel humi...|sadness|    1| didnt feel humil...|[, didnt, feel, h...|[, didnt, feel, h...|(1000,[0,1,46,565...|(1000,[0,1,46,565...|\n",
      "|i can go from fee...|sadness|    1| can  from feelin...|[, can, , from, f...|[, , feeling, , h...|(1000,[0,2,40,51,...|(1000,[0,2,40,51,...|\n",
      "|im grabbing a min...|  anger|    2| grabbing  minute...|[, grabbing, , mi...|[, grabbing, , mi...|(1000,[0,1,173,32...|(1000,[0,1,173,32...|\n",
      "|i am ever feeling...|   love|    4|  ever feeling no...|[, , ever, feelin...|[, , ever, feelin...|(1000,[0,2,5,13,6...|(1000,[0,2,5,13,6...|\n",
      "|i am feeling grouchy|  anger|    2|     feeling grouchy|[, , feeling, gro...|[, , feeling, gro...|(1000,[0,2,955],[...|(1000,[0,2,955],[...|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_test_p2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('text', StringType(), True), StructField('emotion', StringType(), True), StructField('class', IntegerType(), True), StructField('text_c', StringType(), True), StructField('weight', DoubleType(), False)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_weight.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature selection method 3\n",
    "defining the pipeline stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# # Define the preprocessing pipeline\n",
    "# tokenizer = Tokenizer(inputCol=\"text_c\", outputCol=\"words\")\n",
    "# remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "# countVectorizer = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=1000)\n",
    "# idf = IDF(inputCol=countVectorizer.getOutputCol(), outputCol=\"featuresIDF\")\n",
    "\n",
    "\n",
    "\n",
    "# rfe = RandomForestClassifier( numTrees=10, featuresCol=idf.getOutputCol(), labelCol=\"class\")\n",
    "# pipeline_3 = Pipeline(stages=[tokenizer, remover, countVectorizer, idf, rfe])\n",
    "#model3= pipeline_3.fit(df_training_weight)\n",
    "#model3.stages[-1].featureImportances\n",
    "\n",
    "# https://www.timlrx.com/blog/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.types import IntegerType\n",
    "# tokenizer = Tokenizer(inputCol=\"text_c\", outputCol=\"words\",)\n",
    "# remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "# countVectorizer = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=1000)\n",
    "# idf = IDF(inputCol=countVectorizer.getOutputCol(), outputCol=\"featuresIDF\")\n",
    "# lr = LogisticRegression(featuresCol=idf.getOutputCol(), maxIter=100, family=\"multinomial\")\n",
    "# # lr.setLabelCol(\"class\")\n",
    "# # Create the pipeline\n",
    "# pipeline_3 = Pipeline(stages=[tokenizer, remover, countVectorizer, idf, lr])\n",
    "\n",
    "\n",
    "# # Set up the parameter grid for cross-validation\n",
    "# param_grid = ParamGridBuilder() \\\n",
    "#              .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#              .addGrid(lr.elasticNetParam, [0.0, 1.0]) \\\n",
    "#              .build()\n",
    "\n",
    "# # Set up the cross-validator\n",
    "# cv = CrossValidator(estimator=pipeline_3,\n",
    "#                     estimatorParamMaps=param_grid,\n",
    "#                     evaluator=MulticlassClassificationEvaluator(),\n",
    "#                     numFolds=5)\n",
    "# # Fit the CrossValidator to the dataset\n",
    "# df_adjusted = df_training_weight.withColumnRenamed(\"class\", \"label\")\n",
    "# cvModel = cv.fit(df_adjusted)\n",
    "\n",
    "# # # Get the best model from the cross-validation\n",
    "# # best_model = cvModel.bestModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = cvModel.bestModel\n",
    "# # best_model.transform(df_testing_clean)\n",
    "\n",
    "# # Transform the data\n",
    "# transformed_training_p3= best_model.transform(df_training_weight)\n",
    "# transformed_test_p3 = best_model.transform(df_testing_clean)\n",
    "# transformed_val_p3 = best_model.transform(df_val_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_cabfba633998,\n",
       " StopWordsRemover_42f33022cf22,\n",
       " CountVectorizerModel: uid=CountVectorizer_c7cf636ed0b9, vocabularySize=1000,\n",
       " IDFModel: uid=IDF_018d0d95398d, numDocs=16000, numFeatures=1000,\n",
       " LogisticRegressionModel: uid=LogisticRegression_4fc31110dbeb, numClasses=6, numFeatures=1000]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|emotion|class|              text_c|   weight|               words|            filtered|         rawFeatures|         featuresIDF|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|i didnt feel humi...|sadness|    1| didnt feel humil...| 0.291625|[, didnt, feel, h...|[, didnt, feel, h...|(1000,[0,1,46,565...|(1000,[0,1,46,565...|[1.24301753836982...|[0.12357282929290...|       1.0|\n",
      "|i can go from fee...|sadness|    1| can  from feelin...| 0.291625|[, can, , from, f...|[, , feeling, , h...|(1000,[0,2,40,51,...|(1000,[0,2,40,51,...|[2.88263534673151...|[0.45212757569003...|       1.0|\n",
      "|im grabbing a min...|  anger|    2| grabbing  minute...|0.1349375|[, grabbing, , mi...|[, grabbing, , mi...|(1000,[0,1,173,32...|(1000,[0,1,173,32...|[1.24301753836982...|[0.08592852893749...|       2.0|\n",
      "|i am ever feeling...|   love|    4|  ever feeling no...|   0.0815|[, , ever, feelin...|[, , ever, feelin...|(1000,[0,2,5,13,6...|(1000,[0,2,5,13,6...|[1.22807809384940...|[0.09091497228331...|       4.0|\n",
      "|i am feeling grouchy|  anger|    2|     feeling grouchy|0.1349375|[, , feeling, gro...|[, , feeling, gro...|(1000,[0,2,955],[...|(1000,[0,2,955],[...|[1.22807809384940...|[0.15456221503307...|       2.0|\n",
      "+--------------------+-------+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformed_training_p3.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will only use count vectorizer and uncleaned data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Tokenize the review text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "countVectorizer = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=1000)\n",
    "# Create a count vectoriser\n",
    "pipeline_4 = Pipeline(stages=[tokenizer,countVectorizer, idf])\n",
    "data_model_4  = pipeline_4.fit(df_training_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data prep model\n",
    "# Transformation 1 \n",
    "transformed_training_2= data_model_4.transform(df_training_weight)\n",
    "transformed_test_2= data_model_4.transform(df_testing_clean)\n",
    "transformed_val_2= data_model_4.transform(df_val_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_val_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformed_val_2\u001b[39m.\u001b[39mshow(\u001b[39m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformed_val_2' is not defined"
     ]
    }
   ],
   "source": [
    "transformed_val_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_val_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# save as python dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m transformed_val_2\u001b[39m.\u001b[39mto_pickle(\u001b[39m'\u001b[39m\u001b[39mmy_dataframe.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformed_val_2' is not defined"
     ]
    }
   ],
   "source": [
    "# save as python dataset\n",
    "transformed_val_2.to_pickle('my_dataframe.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv_emotions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7781f8813e32d1f6628649145f89b2312aa40072746c548f9037e213846e288f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
