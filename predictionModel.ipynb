{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 13:09:37 WARN Utils: Your hostname, ubuntu-hadoop resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/02/25 13:09:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 13:09:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/25 13:09:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/02/25 13:09:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local[*]\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml import Pipeline\n",
    "#evaluation\n",
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_metrics_l(ml_model,test_data):\n",
    "    predictions = ml_model.transform(test_data).cache()\n",
    "    predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd.map(lambda x: (float(x[0]), float(x[1]))).cache()\n",
    "    \n",
    "    # Print some predictions vs labels\n",
    "    # print(predictionAndLabels.take(10))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    # Overall statistics\n",
    "    precision = metrics.precision(1.0)\n",
    "    recall = metrics.recall(1.0)\n",
    "    f1Score = metrics.fMeasure(1.0)\n",
    "    print(f\"Precision = {precision:.4f} Recall = {recall:.4f} F1 Score = {f1Score:.4f}\")\n",
    "    print(\"Confusion matrix \\n\", metrics.confusionMatrix().toArray().astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|emotion|label|              text_c|               words|            filtered|         rawFeatures|         featuresIDF|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|im feeling rather...|sadness|    1|  feel rather rot...|[, , feel, rather...|[, , feel, rather...|(1000,[0,1,42,102...|(1000,[0,1,42,102...|\n",
      "|im updating my bl...|sadness|    1|  update   blog b...|[, , update, , , ...|[, , update, , , ...|(1000,[0,1,117,34...|(1000,[0,1,117,34...|\n",
      "|i never make her ...|sadness|    1|  never make she ...|[, , never, make,...|[, , never, make,...|(1000,[0,1,2,4,9,...|(1000,[0,1,2,4,9,...|\n",
      "|i left with my bo...|    joy|    0|  leave with   bo...|[, , leave, with,...|[, , leave, , , b...|(1000,[0,1,39,249...|(1000,[0,1,39,249...|\n",
      "|i was feeling a l...|sadness|    1|  be feel   littl...|[, , be, feel, , ...|[, , feel, , , li...|(1000,[0,1,11,17,...|(1000,[0,1,11,17,...|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read the PySpark DataFrame from the Parquet file\n",
    "df_training_m1 = spark.read.parquet(\"data/transformed_training.parquet\")\n",
    "df_testing_m1 = spark.read.parquet(\"data/transformed_test.parquet\")\n",
    "df_val_m1 = spark.read.parquet(\"data/transformed_val.parquet\")\n",
    "df_training_m1.cache()\n",
    "df_testing_m1.cache()\n",
    "df_val_m1.cache()\n",
    "print(df_testing_m1.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 13:11:05 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/02/25 13:11:05 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/02/25 13:11:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/02/25 13:11:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hicran/Documents/emotions-detector/virtualenv_emotions/lib/python3.10/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 164:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8836 Recall = 0.8556 F1 Score = 0.8694\n",
      "Confusion matrix \n",
      " [[671  47  43  30  79  23]\n",
      " [ 22 486  25  19   6  10]\n",
      " [  6   8 205   6   3   2]\n",
      " [  3   7   2 157   1  13]\n",
      " [  2   2   0   0  88   0]\n",
      " [  0   0   0   0   1  33]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "classifier = LinearSVC(maxIter=10, regParam=0.1, featuresCol = \"featuresIDF\", weightCol=\"weight\", labelCol=\"label\")\n",
    "# Define OneVsRest strategy\n",
    "ovr = OneVsRest(classifier=classifier, labelCol=\"label\", featuresCol=\"featuresIDF\", weightCol=\"weight\")\n",
    "pipeline = Pipeline(stages=[ovr])\n",
    "model = pipeline.fit(df_training_m1)\n",
    "m_metrics_l(model,df_val_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"f1\")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.maxIter, [10,50, 100]) \\\n",
    "    .addGrid(classifier.regParam,  [0.01, 0.1, 1.0])\\\n",
    "    .build()\n",
    "    \n",
    "# Define the cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n\u001b[1;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Fit the cross-validator to the training set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cv_model \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49mfit(df_training_m1)\n\u001b[1;32m      4\u001b[0m \u001b[39m# get the best model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m best_model \u001b[39m=\u001b[39m cv_model\u001b[39m.\u001b[39mbestModel\n",
      "File \u001b[0;32m~/Documents/emotions-detector/virtualenv_emotions/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/emotions-detector/virtualenv_emotions/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 861\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    862\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the cross-validator to the training set\n",
    "cv_model = cv.fit(df_training_m1)\n",
    "\n",
    "# get the best model\n",
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.transform(df_testing_m1)\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"f1 on validation set for best model:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv_emotions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7781f8813e32d1f6628649145f89b2312aa40072746c548f9037e213846e288f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
