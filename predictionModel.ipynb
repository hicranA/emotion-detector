{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/26 16:36:02 WARN Utils: Your hostname, ubuntu-hadoop resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/02/26 16:36:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/26 16:36:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local[*]\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml import Pipeline\n",
    "#evaluation\n",
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_metrics_l(ml_model,test_data):\n",
    "    predictions = ml_model.transform(test_data).cache()\n",
    "    predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd.map(lambda x: (float(x[0]), float(x[1]))).cache()\n",
    "    # Print some predictions vs labels\n",
    "    # print(predictionAndLabels.take(10))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    # Overall statistics\n",
    "    precision = metrics.precision(1.0)\n",
    "    recall = metrics.recall(1.0)\n",
    "    f1Score = metrics.fMeasure(1.0)\n",
    "    print(f\"Precision = {precision:.4f} Recall = {recall:.4f} F1 Score = {f1Score:.4f}\")\n",
    "    print(\"Confusion matrix \\n\", metrics.confusionMatrix().toArray().astype(int))\n",
    "    return precision, recall, f1Score, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|emotion|label|              text_c|               words|            filtered|         rawFeatures|         featuresIDF|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|im feeling rather...|sadness|    1|  feel rather rot...|[, , feel, rather...|[, , feel, rather...|(1000,[0,1,42,102...|(1000,[0,1,42,102...|\n",
      "|im updating my bl...|sadness|    1|  update   blog b...|[, , update, , , ...|[, , update, , , ...|(1000,[0,1,117,34...|(1000,[0,1,117,34...|\n",
      "|i never make her ...|sadness|    1|  never make she ...|[, , never, make,...|[, , never, make,...|(1000,[0,1,2,4,9,...|(1000,[0,1,2,4,9,...|\n",
      "|i left with my bo...|    joy|    0|  leave with   bo...|[, , leave, with,...|[, , leave, , , b...|(1000,[0,1,39,249...|(1000,[0,1,39,249...|\n",
      "|i was feeling a l...|sadness|    1|  be feel   littl...|[, , be, feel, , ...|[, , feel, , , li...|(1000,[0,1,11,17,...|(1000,[0,1,11,17,...|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# read the PySpark DataFrame from the Parquet file\n",
    "df_training_m1 = spark.read.parquet(\"data/transformed_training.parquet\")\n",
    "df_testing_m1 = spark.read.parquet(\"data/transformed_test.parquet\")\n",
    "df_val_m1 = spark.read.parquet(\"data/transformed_val.parquet\")\n",
    "df_training_m1.cache()\n",
    "df_testing_m1.cache()\n",
    "df_val_m1.cache()\n",
    "print(df_testing_m1.show(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM - OneVSrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def liniearSVMMaker(df, df_val):\n",
    "    classifier = LinearSVC(maxIter=10, regParam=0.1, featuresCol = \"featuresIDF\", weightCol=\"weight\", labelCol=\"label\")\n",
    "    # Define OneVsRest strategy\n",
    "    ovr = OneVsRest(classifier=classifier, labelCol=\"label\", featuresCol=\"featuresIDF\", weightCol=\"weight\")\n",
    "    start = time.time()\n",
    "    pipeline = Pipeline(stages=[ovr])\n",
    "    model = pipeline.fit(df)\n",
    "    training_time = time.time()-start\n",
    "    precision_svm, recall_svm , f1Score_svm,  metrics = m_metrics_l(model,df_val)\n",
    "    return precision_svm, recall_svm, f1Score_svm, metrics, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/26 15:32:29 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/02/26 15:32:29 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/26 15:32:29 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/02/26 15:32:29 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hicran/Documents/emotions-detector/virtualenv_emotions/lib/python3.10/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 164:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8836 Recall = 0.8556 F1 Score = 0.8694\n",
      "Confusion matrix \n",
      " [[671  47  43  30  79  23]\n",
      " [ 22 486  25  19   6  10]\n",
      " [  6   8 205   6   3   2]\n",
      " [  3   7   2 157   1  13]\n",
      " [  2   2   0   0  88   0]\n",
      " [  0   0   0   0   1  33]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "precision_svm, recall_svm, f1Score_svm, metrics, training_time_svm = liniearSVMMaker(df= df_training_m1, df_val=df_val_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.2072913646698"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_time_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(type(training_time_svm))\n",
    "print(type(precision_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------------+---------+------+-------+\n",
      "|          dataModel|modelName|trainingTime|precision|recall|f1Score|\n",
      "+-------------------+---------+------------+---------+------+-------+\n",
      "|countVectorizer+iDF|      svm|       42.21|     0.88|  0.86|   0.87|\n",
      "+-------------------+---------+------------+---------+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results_df = spark.createDataFrame([(\"countVectorizer+iDF\",\n",
    "                                     \"svm\", \n",
    "                                    round(training_time_svm,2),\n",
    "                                    round(precision_svm,2), \n",
    "                                     round(recall_svm,2), \n",
    "                                     round(f1Score_svm,2))], \n",
    "                                     [\"dataModel\", \"modelName\", \"trainingTime\", \"precision\",\"recall\", \"f1Score\"])\n",
    "\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "def logRegmaker(df, df_val):\n",
    "    cassifier = LogisticRegression(maxIter=10, regParam=0.1, featuresCol = \"featuresIDF\", weightCol=\"weight\")\n",
    "    start = time.time()\n",
    "    pipeline = Pipeline(stages=[cassifier])\n",
    "    print(f\"Training started.\")\n",
    "    model = pipeline.fit(df)\n",
    "    training_time = time.time()-start\n",
    "    precision, recall , f1Score,  metrics = m_metrics_l(model,df_val)\n",
    "    return precision, recall, f1Score, metrics, training_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8964 Recall = 0.8016 F1 Score = 0.8464\n",
      "Confusion matrix \n",
      " [[678  47  47  32  74  31]\n",
      " [ 21 493  39  28  18  16]\n",
      " [  3   4 188   6   4   1]\n",
      " [  1   4   1 146   1  10]\n",
      " [  1   2   0   0  80   0]\n",
      " [  0   0   0   0   1  23]]\n"
     ]
    }
   ],
   "source": [
    "precision_lr, recall_lr , f1Score_lr , metrics_lr, training_time_lr = logRegmaker(df= df_training_m1, df_val=df_val_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= results_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df.index)]= [\"countVectorizer+iDF\",\n",
    "                                     \"lr\", training_time_lr, precision_lr, recall_lr, f1Score_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataModel</th>\n",
       "      <th>modelName</th>\n",
       "      <th>trainingTime</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>countVectorizer+iDF</td>\n",
       "      <td>svm</td>\n",
       "      <td>42.210000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>countVectorizer+iDF</td>\n",
       "      <td>lr</td>\n",
       "      <td>3.403402</td>\n",
       "      <td>0.896364</td>\n",
       "      <td>0.801626</td>\n",
       "      <td>0.846352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataModel modelName  trainingTime  precision    recall   f1Score\n",
       "0  countVectorizer+iDF       svm     42.210000   0.880000  0.860000  0.870000\n",
       "1  countVectorizer+iDF        lr      3.403402   0.896364  0.801626  0.846352"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "def nBMaker(df, df_val):\n",
    "    nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol = \"featuresIDF\", weightCol=\"weight\")\n",
    "    # nb_model = nb.fit(df_training_m1)\n",
    "    start = time.time()\n",
    "    pipeline = Pipeline(stages=[nb])\n",
    "    print(f\"Training started.\")\n",
    "    model = pipeline.fit(df)\n",
    "    training_time = time.time()-start\n",
    "    precision, recall , f1Score,  metrics = m_metrics_l(model,df_val)\n",
    "    return precision, recall, f1Score, metrics, training_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hicran/Documents/emotions-detector/virtualenv_emotions/lib/python3.10/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8527 Recall = 0.8257 F1 Score = 0.8390\n",
      "Confusion matrix \n",
      " [[586  23  21   6  30   7]\n",
      " [ 34 469  29  17  11   8]\n",
      " [ 14  15 209   8   5   2]\n",
      " [ 23  16  10 169   3  13]\n",
      " [ 35  16   3   4 128   0]\n",
      " [ 12  11   3   8   1  51]]\n"
     ]
    }
   ],
   "source": [
    "precision_nb, recall_nb, f1Score_nb, metrics_nb, training_time_nb = nBMaker(df=df_training_m1, df_val=df_val_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df.index)]= [\"countVectorizer+iDF\",\n",
    "                                     \"nb\", training_time_nb, precision_nb, recall_nb, f1Score_nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataModel</th>\n",
       "      <th>modelName</th>\n",
       "      <th>trainingTime</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>countVectorizer+iDF</td>\n",
       "      <td>svm</td>\n",
       "      <td>42.210000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>countVectorizer+iDF</td>\n",
       "      <td>lr</td>\n",
       "      <td>3.403402</td>\n",
       "      <td>0.896364</td>\n",
       "      <td>0.801626</td>\n",
       "      <td>0.846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>countVectorizer+iDF</td>\n",
       "      <td>nb</td>\n",
       "      <td>2.511171</td>\n",
       "      <td>0.852727</td>\n",
       "      <td>0.825704</td>\n",
       "      <td>0.838998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataModel modelName  trainingTime  precision    recall   f1Score\n",
       "0  countVectorizer+iDF       svm     42.210000   0.880000  0.860000  0.870000\n",
       "1  countVectorizer+iDF        lr      3.403402   0.896364  0.801626  0.846352\n",
       "2  countVectorizer+iDF        nb      2.511171   0.852727  0.825704  0.838998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import time\n",
    "#     classifier = LinearSVC(maxIter=10, regParam=0.1, featuresCol = \"featuresIDF\", weightCol=\"weight\", labelCol=\"label\")\n",
    "#     # Define OneVsRest strategy\n",
    "#     ovr = OneVsRest(classifier=classifier, labelCol=\"label\", featuresCol=\"featuresIDF\", weightCol=\"weight\")\n",
    "#     pipeline = Pipeline(stages=[ovr])\n",
    "#     model = pipeline.fit(df_training_m1)\n",
    "#     m_metrics_l(model,df_val_m1)\n",
    "\n",
    "#     evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"f1\")\n",
    "\n",
    "#     # Define the hyperparameter grid\n",
    "#     param_grid = ParamGridBuilder() \\\n",
    "#      .addGrid(classifier.maxIter, [10, 50, 100]) \\\n",
    "#      .addGrid(classifier.regParam,  [0.01, 0.1, 1.0])\\\n",
    "#         .build()\n",
    "    \n",
    "#     # Define the cross-validator\n",
    "#     cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "#     # Fit the cross-validator to the training set\n",
    "#     cv_model = cv.fit(df_training_m1)\n",
    "\n",
    "#     # Evaluate the best model on the validation set\n",
    "#     best_model = cv_model.bestModel\n",
    "#     predictions = best_model.transform(df_testing_m1)\n",
    "#     f1 = evaluator.evaluate(predictions)\n",
    "#     print(\"f1 on validation set for best model:\", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/26 17:17:23 WARN CacheManager: Asked to cache already cached data.\n",
      "23/02/26 17:17:23 WARN CacheManager: Asked to cache already cached data.\n",
      "23/02/26 17:17:23 WARN CacheManager: Asked to cache already cached data.\n",
      "Column<'label'>\n"
     ]
    }
   ],
   "source": [
    "df_training_m2 = spark.read.parquet(\"data/transformed_training_p2.parquet\")\n",
    "df_testing_m2 = spark.read.parquet(\"data/transformed_test_p2.parquet\")\n",
    "df_val_m2 = spark.read.parquet(\"data/transformed_val_p2.parquet\")\n",
    "df_training_m2.cache()\n",
    "df_testing_m2.cache()\n",
    "df_val_m2.cache()\n",
    "print(df_testing_m2.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "classifier = LinearSVC(maxIter=10, regParam=0.1, featuresCol = \"features\", weightCol=\"weight\", labelCol=\"label\")\n",
    "# Define OneVsRest strategy\n",
    "ovr = OneVsRest(classifier=classifier, labelCol=\"label\", featuresCol=\"features\", weightCol=\"weight\")\n",
    "\n",
    "pipeline = Pipeline(stages=[ovr])\n",
    "model = pipeline.fit(df_training_m2)\n",
    "\n",
    "# m_metrics_l(model,df_val_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hicran/Documents/emotions-detector/virtualenv_emotions/lib/python3.10/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.1527 Recall = 0.5091 F1 Score = 0.2350\n",
      "Confusion matrix \n",
      " [[653 439 153  85 146  39]\n",
      " [ 20  84  22  20   5  14]\n",
      " [ 11   8  97   7   5   3]\n",
      " [ 17  16   3 100   1  13]\n",
      " [  3   2   0   0  21   0]\n",
      " [  0   1   0   0   0  12]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.15272727272727274,\n",
       " 0.509090909090909,\n",
       " 0.234965034965035,\n",
       " <pyspark.mllib.evaluation.MulticlassMetrics at 0x7fc0b88cbf40>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_metrics_l(model,df_val_m2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
